{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHcsLgK87JeQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "qMS1b_A-7NuM",
        "outputId": "9e988186-1aa8-42c5-9ba0-d9b5dfa64e40"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9bdf261f-1872-4de1-b2ab-3d3d7995cbfc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9bdf261f-1872-4de1-b2ab-3d3d7995cbfc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving MyPersonality - personality_data - MyPersonality - personality_data.csv to MyPersonality - personality_data - MyPersonality - personality_data (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files=files.upload()\n",
        "data = pd.read_csv('MyPersonality - personality_data - MyPersonality - personality_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP87_V7n7NzD",
        "outputId": "18c945a4-1944-4288-e5e6-0ca4c874b2bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing for data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\n",
        "\n",
        "def extract_label(label):\n",
        "    if pd.isna(label):\n",
        "        return None\n",
        "    return label.split(':')[-1]\n",
        "\n",
        "label_columns = ['O', 'C', 'E', 'A', 'N']\n",
        "for col in label_columns:\n",
        "    data[col] = data[col].apply(extract_label)\n",
        "    data[col] = data[col].map({'y': 1, 'n': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcGGVOgV7N1T",
        "outputId": "12acdf3f-bb7c-47bd-e929-cb1369e1cce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows: 9917\n",
            "text            0\n",
            "O               0\n",
            "C               0\n",
            "E               0\n",
            "A               0\n",
            "N               0\n",
            "cleaned_text    0\n",
            "dtype: int64\n",
            "\n",
            "Show Data After clean:\n",
            "                                        cleaned_text  O  C  E  A  N\n",
            "0                            got new phone christmas  0  0  0  0  1\n",
            "1  caught fireflies jar going soak feet piping ho...  1  1  0  0  0\n",
            "2                                     happy new year  0  0  1  0  0\n",
            "3                           ewwwwwwwwwwwwwwwwwwwwwww  1  1  1  0  0\n",
            "4  wondering heck last week computer hard drive d...  0  1  0  1  0\n",
            "5  might almost like bridgets cinnamon cardamom b...  1  0  1  1  0\n",
            "6     wondering bad mom since kids still watching tv  0  0  0  1  0\n",
            "7  artzone san francisco jan feb seduction ducham...  1  0  0  1  0\n",
            "8                       gon na alright tonight night  0  1  0  1  0\n",
            "9  hal alive time conveniently enough sure im gon...  1  0  0  1  0\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of rows:\", len(data))\n",
        "print(data.isnull().sum())\n",
        "print(\"\\nShow Data After clean:\")\n",
        "print(data[['cleaned_text', 'O', 'C', 'E', 'A', 'N']].head(10))\n",
        "# Add synthetic data to balance all traits\n",
        "synthetic_texts = [\n",
        "    # Openness (O)\n",
        "    (\"I love exploring new ideas and trying new experiences.\", 1, 0, 0, 0, 0),\n",
        "    (\"I’m always eager to dive into creative projects.\", 1, 0, 0, 0, 0),\n",
        "    # Conscientiousness (C)\n",
        "    (\"I plan everything in advance and keep my life organized.\", 0, 1, 0, 0, 0),\n",
        "    (\"I always complete my tasks on time without delays.\", 0, 1, 0, 0, 0),\n",
        "    # Extraversion (E)\n",
        "    (\"I enjoy being around people and feel energized at parties.\", 0, 0, 1, 0, 0),\n",
        "    (\"I love talking to everyone and being the center of attention.\", 0, 0, 1, 0, 0),\n",
        "    # Agreeableness (A)\n",
        "    (\"I always help others and avoid conflicts at all costs.\", 0, 0, 0, 1, 0),\n",
        "    (\"I make sure everyone around me feels happy and supported.\", 0, 0, 0, 1, 0),\n",
        "    # Neuroticism (N)\n",
        "    (\"I feel anxious all the time and worry about everything.\", 0, 0, 0, 0, 1),\n",
        "    (\"I often get overwhelmed by my emotions for no reason.\", 0, 0, 0, 0, 1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBddkWge9Vxc",
        "outputId": "94c43e76-5085-4d61-ec59-fb8852dbef2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribution of O:\n",
            "O\n",
            "1    7374\n",
            "0    2563\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of C:\n",
            "C\n",
            "0    5377\n",
            "1    4560\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of E:\n",
            "E\n",
            "0    5723\n",
            "1    4214\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of A:\n",
            "A\n",
            "1    5272\n",
            "0    4665\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of N:\n",
            "N\n",
            "0    6216\n",
            "1    3721\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "for col in label_columns:\n",
        "    print(f\"\\nDistribution of {col}:\")\n",
        "    print(data[col].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMCSqByS7N37",
        "outputId": "4fb10aa9-ec60-4756-8087-da47a9da0c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape after adding synthetic data: (9947, 7)\n",
            "Distribution of O after synthetic data:\n",
            "O\n",
            "1    7376\n",
            "0    2571\n",
            "Name: count, dtype: int64\n",
            "Distribution of C after synthetic data:\n",
            "C\n",
            "0    5385\n",
            "1    4562\n",
            "Name: count, dtype: int64\n",
            "Distribution of E after synthetic data:\n",
            "E\n",
            "0    5731\n",
            "1    4216\n",
            "Name: count, dtype: int64\n",
            "Distribution of A after synthetic data:\n",
            "A\n",
            "1    5274\n",
            "0    4673\n",
            "Name: count, dtype: int64\n",
            "Distribution of N after synthetic data:\n",
            "N\n",
            "0    6224\n",
            "1    3723\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame for synthetic data\n",
        "synthetic_data = pd.DataFrame(\n",
        "    synthetic_texts,\n",
        "    columns=['cleaned_text', 'O', 'C', 'E', 'A', 'N']\n",
        ")\n",
        "\n",
        "# Keep a copy of the original data\n",
        "data_original = pd.concat([data, synthetic_data], ignore_index=True)\n",
        "print(f\"Data shape after adding synthetic data: {data_original.shape}\")\n",
        "for col in label_columns:\n",
        "    print(f\"Distribution of {col} after synthetic data:\")\n",
        "    print(data_original[col].value_counts(dropna=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbMprBkU7N6c",
        "outputId": "dac5c849-4066-4071-f098-da6afa351eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribution of O before oversampling:\n",
            "O\n",
            "1    7357\n",
            "0    2550\n",
            "Name: count, dtype: int64\n",
            "Oversample ratio for O: 1\n",
            "\n",
            "Distribution of O after oversampling:\n",
            "O\n",
            "1    7357\n",
            "0    2550\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of C before oversampling:\n",
            "C\n",
            "0    5357\n",
            "1    4550\n",
            "Name: count, dtype: int64\n",
            "Oversample ratio for C: 1\n",
            "\n",
            "Distribution of C after oversampling:\n",
            "C\n",
            "1    5357\n",
            "0    5357\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of E before oversampling:\n",
            "E\n",
            "0    6072\n",
            "1    4642\n",
            "Name: count, dtype: int64\n",
            "Oversample ratio for E: 1\n",
            "\n",
            "Distribution of E after oversampling:\n",
            "E\n",
            "1    6072\n",
            "0    6072\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of A before oversampling:\n",
            "A\n",
            "1    6609\n",
            "0    5535\n",
            "Name: count, dtype: int64\n",
            "Oversample ratio for A: 1\n",
            "\n",
            "Distribution of A after oversampling:\n",
            "A\n",
            "1    6609\n",
            "0    5535\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of N before oversampling:\n",
            "N\n",
            "0    7976\n",
            "1    4168\n",
            "Name: count, dtype: int64\n",
            "Oversample ratio for N: 1\n",
            "\n",
            "Distribution of N after oversampling:\n",
            "N\n",
            "0    7976\n",
            "1    7976\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Final distribution after sequential oversampling:\n",
            "\n",
            "Distribution of O:\n",
            "O\n",
            "1    11581\n",
            "0     4371\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of C:\n",
            "C\n",
            "0    8258\n",
            "1    7694\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of E:\n",
            "E\n",
            "0    8929\n",
            "1    7023\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of A:\n",
            "A\n",
            "0    8083\n",
            "1    7869\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of N:\n",
            "N\n",
            "0    7976\n",
            "1    7976\n",
            "Name: count, dtype: int64\n",
            "Total rows: 15952\n",
            "Training rows: 12761\n",
            "Testing rows: 3191\n"
          ]
        }
      ],
      "source": [
        "# === Oversampling Sequentially for ALL Traits ===\n",
        "for target_trait in label_columns:\n",
        "    print(f\"\\nDistribution of {target_trait} before oversampling:\")\n",
        "    print(data[target_trait].value_counts())\n",
        "\n",
        "    minority_class = data[data[target_trait] == 1]\n",
        "    majority_class = data[data[target_trait] == 0]\n",
        "\n",
        "    num_majority = len(majority_class)\n",
        "    num_minority = len(minority_class)\n",
        "\n",
        "    if num_minority == 0:\n",
        "        print(f\"No minority class (value=1) found for {target_trait}. Skipping oversampling for this trait.\")\n",
        "        continue\n",
        "\n",
        "    # Aim for 50/50 balance\n",
        "    oversample_ratio = max(1, num_majority // num_minority)\n",
        "    print(f\"Oversample ratio for {target_trait}: {oversample_ratio}\")\n",
        "\n",
        "    minority_oversampled = pd.concat([minority_class] * oversample_ratio, ignore_index=True)\n",
        "\n",
        "    remaining_samples = num_majority - len(minority_oversampled)\n",
        "    if remaining_samples > 0:\n",
        "        additional_samples = minority_class.sample(n=remaining_samples, replace=True, random_state=42)\n",
        "        minority_oversampled = pd.concat([minority_oversampled, additional_samples], ignore_index=True)\n",
        "\n",
        "    # Update data with the oversampled version\n",
        "    data = pd.concat([majority_class, minority_oversampled], ignore_index=True)\n",
        "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\nDistribution of {target_trait} after oversampling:\")\n",
        "    print(data[target_trait].value_counts())\n",
        "\n",
        "# Check final distribution after oversampling\n",
        "print(\"\\nFinal distribution after sequential oversampling:\")\n",
        "for col in label_columns:\n",
        "    print(f\"\\nDistribution of {col}:\")\n",
        "    print(data[col].value_counts(dropna=False))\n",
        "\n",
        "# Split the data\n",
        "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
        "print(f\"Total rows: {len(data)}\")\n",
        "print(f\"Training rows: {len(train_df)}\")\n",
        "print(f\"Testing rows: {len(test_df)}\")\n",
        "\n",
        "\n",
        "trait_columns = ['O', 'C', 'E', 'A', 'N']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td4BS-1h7N9G",
        "outputId": "0c8ebbb9-2eb5-4e09-e618-2f8e1e62261b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Setup tokenizer and device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the dataset class\n",
        "class PersonalityDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = PersonalityDataset(\n",
        "    train_df['cleaned_text'].tolist(),\n",
        "    train_df[trait_columns].values.tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = PersonalityDataset(\n",
        "    test_df['cleaned_text'].tolist(),\n",
        "    test_df[trait_columns].values.tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=len(trait_columns),\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "model.config.hidden_dropout_prob = 0.4\n",
        "model.config.attention_probs_dropout_prob = 0.4\n",
        "model.to(device)\n",
        "\n",
        "# # Load the saved model weights\n",
        "# model_path = '/content/drive/MyDrive/Bert_person_improve_retrained_lastVersion#1.pth'\n",
        "# try:\n",
        "#     model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "#     print(\"Model loaded successfully!\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error loading model: {e}\")\n",
        "#     raise\n",
        "\n",
        "# Training setup\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSUNvWrH7N_k",
        "outputId": "d138b31e-6ece-4c71-cf4e-2514fe3e2185"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-600d2b02ecb3>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(labels, dtype=torch.float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 0.6593, Train Accuracy: 0.5897\n",
            "Epoch 2, Train Loss: 0.5767, Train Accuracy: 0.6960\n",
            "Epoch 3, Train Loss: 0.4360, Train Accuracy: 0.8007\n",
            "Epoch 4, Train Loss: 0.3157, Train Accuracy: 0.8687\n",
            "Epoch 5, Train Loss: 0.2247, Train Accuracy: 0.9136\n",
            "Epoch 6, Train Loss: 0.1585, Train Accuracy: 0.9434\n",
            "Epoch 7, Train Loss: 0.1130, Train Accuracy: 0.9605\n",
            "Epoch 8, Train Loss: 0.0869, Train Accuracy: 0.9705\n",
            "Epoch 9, Train Loss: 0.0682, Train Accuracy: 0.9767\n",
            "Epoch 10, Train Loss: 0.0566, Train Accuracy: 0.9797\n",
            "Epoch 11, Train Loss: 0.0470, Train Accuracy: 0.9834\n",
            "Epoch 12, Train Loss: 0.0410, Train Accuracy: 0.9850\n",
            "Epoch 13, Train Loss: 0.0384, Train Accuracy: 0.9859\n",
            "Epoch 14, Train Loss: 0.0337, Train Accuracy: 0.9873\n",
            "Epoch 15, Train Loss: 0.0343, Train Accuracy: 0.9863\n",
            "Epoch 16, Train Loss: 0.0330, Train Accuracy: 0.9871\n"
          ]
        }
      ],
      "source": [
        "# Continue training the loaded model\n",
        "for epoch in range(16):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        predictions = (torch.sigmoid(outputs.logits) > 0.5).float()\n",
        "        train_correct += (predictions == labels).sum().item()\n",
        "        train_total += labels.numel()\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_accuracy = train_correct / train_total\n",
        "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyP1LHkf7OB7",
        "outputId": "93c307af-09cf-4832-9d43-803d05e7a994"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-600d2b02ecb3>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(labels, dtype=torch.float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Test Phase ===\n",
            "Average Test Loss: 0.8763\n",
            "\n",
            "Accuracy per trait:\n",
            "O: 0.8734\n",
            "C: 0.8211\n",
            "E: 0.8233\n",
            "A: 0.7979\n",
            "N: 0.8314\n",
            "\n",
            "Overall Test Accuracy: 0.8294\n"
          ]
        }
      ],
      "source": [
        "# === Test Phase ===\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs.logits)\n",
        "        all_probs.append(probabilities.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "# Concatenate all probabilities and labels\n",
        "all_probs = np.concatenate(all_probs, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "# Calculate average test loss\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(\"\\n=== Test Phase ===\")\n",
        "print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "# Apply default thresholds to get binary predictions\n",
        "best_thresholds = [0.5] * len(trait_columns)\n",
        "test_preds = np.zeros_like(all_probs)\n",
        "for i, thresh in enumerate(best_thresholds):\n",
        "    test_preds[:, i] = (all_probs[:, i] > thresh).astype(float)\n",
        "\n",
        "# Calculate accuracy per trait\n",
        "print(\"\\nAccuracy per trait:\")\n",
        "for i, trait in enumerate(trait_columns):\n",
        "    trait_acc = accuracy_score(all_labels[:, i], test_preds[:, i])\n",
        "    print(f\"{trait}: {trait_acc:.4f}\")\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(all_labels.flatten(), test_preds.flatten())\n",
        "print(f\"\\nOverall Test Accuracy: {overall_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z52gIVJV7OEj",
        "outputId": "07294572-bcfd-4d48-a84b-af415ae40960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Full Dataset Accuracy ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-600d2b02ecb3>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(labels, dtype=torch.float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Full Dataset Loss: 0.1878\n",
            "\n",
            "Accuracy per trait (Full Dataset):\n",
            "O: 0.9707\n",
            "C: 0.9583\n",
            "E: 0.9588\n",
            "A: 0.9534\n",
            "N: 0.9598\n",
            "\n",
            "Overall Full Dataset Accuracy: 0.9602\n"
          ]
        }
      ],
      "source": [
        "# === Full Dataset Accuracy ===\n",
        "print(\"\\n=== Full Dataset Accuracy ===\")\n",
        "\n",
        "# Create dataset for the full data\n",
        "full_dataset = PersonalityDataset(\n",
        "    data['cleaned_text'].tolist(),\n",
        "    data[trait_columns].values.tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "# DataLoader for full dataset\n",
        "full_loader = DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Evaluate on full dataset\n",
        "model.eval()\n",
        "full_loss = 0\n",
        "all_full_probs = []\n",
        "all_full_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in full_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        full_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs.logits)\n",
        "        all_full_probs.append(probabilities.cpu().numpy())\n",
        "        all_full_labels.append(labels.cpu().numpy())\n",
        "\n",
        "# Concatenate all probabilities and labels\n",
        "all_full_probs = np.concatenate(all_full_probs, axis=0)\n",
        "all_full_labels = np.concatenate(all_full_labels, axis=0)\n",
        "\n",
        "# Calculate average full dataset loss\n",
        "avg_full_loss = full_loss / len(full_loader)\n",
        "print(f\"Average Full Dataset Loss: {avg_full_loss:.4f}\")\n",
        "\n",
        "# Apply thresholds to get binary predictions\n",
        "full_preds = np.zeros_like(all_full_probs)\n",
        "for i, thresh in enumerate(best_thresholds):\n",
        "    full_preds[:, i] = (all_full_probs[:, i] > thresh).astype(float)\n",
        "\n",
        "# Calculate accuracy per trait for full dataset\n",
        "print(\"\\nAccuracy per trait (Full Dataset):\")\n",
        "for i, trait in enumerate(trait_columns):\n",
        "    trait_acc = accuracy_score(all_full_labels[:, i], full_preds[:, i])\n",
        "    print(f\"{trait}: {trait_acc:.4f}\")\n",
        "\n",
        "# Calculate overall accuracy for full dataset\n",
        "overall_full_accuracy = accuracy_score(all_full_labels.flatten(), full_preds.flatten())\n",
        "print(f\"\\nOverall Full Dataset Accuracy: {overall_full_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOyBck6v7OG9",
        "outputId": "143c01af-8578-4101-e7fb-298e6d79d8f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter any sentence about yourself, your thoughts, or behavior:\n",
            "→ I feel anxious all the time and worry about everything.\n",
            "\n",
            "🧠 Analyze Your Personality Traits:\n",
            "************************************\n",
            "• O: 98.05% (Yes)\n",
            "• C: 1.33% (No)\n",
            "• E: 0.00% (No)\n",
            "• A: 97.63% (Yes)\n",
            "• N: 98.84% (Yes)\n",
            "************************************\n",
            "Your Personality Tends To: O\n"
          ]
        }
      ],
      "source": [
        "# === User Input and Personality Analysis ===\n",
        "def analyze_personality(text, model, tokenizer, trait_columns, device, thresholds):\n",
        "    model.eval()\n",
        "    cleaned_text = clean_text(text)\n",
        "    encoding = tokenizer(\n",
        "        cleaned_text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits.cpu().numpy()[0]\n",
        "\n",
        "    probabilities = torch.sigmoid(torch.tensor(logits)).numpy() * 100\n",
        "    binary_preds = [(probabilities[i] / 100 > thresholds[i]) for i in range(len(trait_columns))]\n",
        "    result = {\n",
        "        trait: f\"{prob:.2f}% ({'Yes' if binary else 'No'})\"\n",
        "        for trait, prob, binary in zip(trait_columns, probabilities, binary_preds)\n",
        "    }\n",
        "\n",
        "# Modified: Select the dominant trait based on the highest probability among \"Yes\" traits\n",
        "    if any(binary_preds):\n",
        "        # Create a list of probabilities, but set to 0 for traits that are \"No\"\n",
        "        adjusted_probs = [prob if binary else 0 for prob, binary in zip(probabilities, binary_preds)]\n",
        "        max_trait_idx = np.argmax(adjusted_probs)\n",
        "        max_trait = trait_columns[max_trait_idx]\n",
        "        return result, binary_preds, max_trait\n",
        "    else:\n",
        "        return result, binary_preds, None\n",
        "\n",
        "# Set default thresholds\n",
        "best_thresholds = [0.5] * len(trait_columns)\n",
        "# Prompt user for input\n",
        "print(\"\\nEnter any sentence about yourself, your thoughts, or behavior:\")\n",
        "user_input = input(\"→ \")\n",
        "#ده الكود الجديد علشان يغير اختيار السمة المهيمنه بناءا علي اعلي نسبة\n",
        "\n",
        "try:\n",
        "    result, binary_preds, max_trait = analyze_personality(user_input, model, tokenizer, trait_columns, device, best_thresholds)\n",
        "\n",
        "    print(\"\\n🧠 Analyze Your Personality Traits:\")\n",
        "    print(\"************************************\")\n",
        "    for trait, prediction in result.items():\n",
        "        print(f\"• {trait}: {prediction}\")\n",
        "    print(\"************************************\")\n",
        "\n",
        "    if max_trait:\n",
        "        print(f\"Your Personality Tends To: {max_trait}\")\n",
        "    else:\n",
        "        print(\"No dominant trait detected with current input.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGIixP1m7OJk",
        "outputId": "aacc48b7-01e6-4540-8be9-713279777afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully after retraining\n"
          ]
        }
      ],
      "source": [
        "# Save the model after retraining\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Bert_person_improve_retrained_lastVersion#2.pth')\n",
        "print(\"Model saved successfully after retraining\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
