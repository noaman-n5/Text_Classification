# Text_Classification
i used Bert model and framework PyTorch for text classification , to analyze personality traits
#ğŸ§  Personality Trait Detection using BERT
This project uses **BERT (Bidirectional Encoder Representations from Transformers)** for multi-label classification to detect Big Five personality traits (OCEAN model) from text input.

## ğŸ” Objective
Build a robust NLP pipeline that:
- Cleans and preprocesses user-generated text
- Balances imbalanced trait labels using synthetic data and oversampling
- Trains a multi-label classification model using `bert-base-uncased`
- Analyzes and outputs personality trait probabilities
- Predicts the **dominant personality trait** based on the highest probability

## ğŸ§ª Personality Traits (OCEAN)
- **O**: Openness
- **C**: Conscientiousness
- **E**: Extraversion
- **A**: Agreeableness
- **N**: Neuroticism

## ğŸ“ Dataset
Upload your CSV file with the following format:

| text | O | C | E | A | N |
|------|---|---|---|---|---|
|"text"| y | n | y | n | y |

### Notes:
- The labels must be strings like `O:y`, `C:n`, etc.
- The script will clean and extract the binary labels from this format.
---

## âš™ï¸ Setup Instructions and training model
### 1. Install Dependencies
```bash
pip install torch transformers nltk pandas numpy matplotlib seaborn scikit-learn imbalanced-learn
2. Download NLTK Resources
import nltk
nltk.download('punkt')
nltk.download('stopwords')
3. Upload Dataset (in Google Colab)
from google.colab import files
files.upload()

##ğŸ§¼Data Preprocessing
Clean and tokenize text
Remove punctuation, digits
Add synthetic balanced samples per trait
Apply oversampling sequentially for each trait to balance class distribution

ğŸ¤– Model Architecture and hyperparameter
BertForSequenceClassification with 5 output nodes for O, C, E, A, N
Multi-label classification using BCEWithLogitsLoss
Optimizer: AdamW
Dropout: 0.4
Trained for 16 epochs

ğŸ“Š Evaluation
Calculates accuracy per trait
Calculate Evaluation Metric
Prints test and full dataset accuracy
Uses threshold of 0.5 for classification


##ğŸ§  Personality Analysis from User Input
The final section allows users to input a sentence.
--The model:
 Preprocesses the text
 Predicts trait probabilities
 Identifies dominant personality trait (highest probability among positive predictions)

ğŸ’¾ Save & Load Model
# Save
torch.save(model.state_dict(), 'model_path.pth')
# Load
model.load_state_dict(torch.load('model_path.pth'))

ğŸ§ª Example
text:
â†’ I love trying new experiences and exploring different cultures.
ğŸ§  Analyze Your Personality Traits:
************************************
â€¢ O: 85.13% (Yes)
â€¢ C: 33.29% (No)
â€¢ E: 58.92% (Yes)
â€¢ A: 42.17% (No)
â€¢ N: 28.94% (No)
************************************
Your Personality Tends To: O
ğŸ“ File Structure
â”œâ”€â”€ dataset.csv
â”œâ”€â”€ personality_analysis.ipynb
â”œâ”€â”€ model.pth
â”œâ”€â”€ README.md
ğŸ“Œ Future Improvements
Use a fine-tuned BERT variant (e.g., RoBERTa)
Add user interface with Streamlit or Flask
Improve dominant trait logic using fuzzy thresholds or dynamic cutoffs
Evaluate with additional personality datasets

ğŸ§‘â€ğŸ’» Author
Developed with â¤ï¸ using PyTorch and HuggingFace by [Abdulrahman noaman]
